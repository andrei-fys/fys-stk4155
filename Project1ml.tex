\documentclass[10pt]{article}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{flexisym}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{subcaption}
\newtheorem*{theorem}{Theorem}
\newtheorem{defn}{Definition}
\begin{document}
\setlength\parindent{1pt}
\title{Data Analysis and Machine Learning \\
Project1.\\ Regression analysis and re-sampling methods}
\author{Andrei Kukharenka, Anton Fofanov and Anna Gribkovskaya \\  
FYS-STK 4155 
}

\maketitle
\begin{abstract}
In this project we have performed the Ordinary Least Squares (OSL), Ridge and Lasso regression. The solvers for OSL and Ridge have been developed from scratch and the Lasso regression have been implemented using a Sci-Kit learn library. The estimators were used on the test Franke function (REF) and on the real data - terrain data for Norway.The k-fold cross validation for all the methods have been also implemented and the bias-variance was studied for all the models.\\
ADD SOME RESULTS HERE!!!!!
\end{abstract}
\clearpage 


\section*{Introduction}
Regression analysis is a widely used tool in data science. It incorporates many different techniques and models for estimating relations between variables. In this project we aim to study most widely used estimators - OSL, Ridge and Lasso.
OSL regression is one of the most popular models here. It has been implemented in many software packages is various programming languages and is used by many data scientist everyday for performing analysis. In this project we aim to implement it from scratch in order to have a better understanding of its machinery. Rigde regression is also implemented from scratch here. \\
A regression analysis aims to find the relations between various variables not only to explain or extract a functional dependency, but also to be able to predict outcomes for some unknown values of this variables. Here we need to be very careful with our model, because even if we were able to implement it with the smallest possible error and even hit all the variables provided for estimation of the relations, we can't be sure it will provide same good results when applied to the new set of data. The problem here is a possibility of over-fitting.In order to prevent this we use so-called re-sampling and cross-validation. In this project a k-fold cross validation have beed applied for all three methods. ADD SOME RESULTS HERE!!!\\
Structure of the report:\\
In section $\ref{Theory}$ we provide a brief theoretical review of the implemented models and re-sampling techniques.\\

\section{General description of the linear regression methods} \label{Theory}
Regression analysis is a powerful tool to analyze the data. Let's start with some definitions needed to understand how it works.
\begin{defn}
Variables $\hat{x} = [x_0,x_1, x_2,\dots, x_{n-1}]^T,$ are called independent  or explanatory variable. Here $n$ is number of samples measured.
\end{defn}

\begin{defn}
	Variables  $\hat{y} = [y_0,y_1, y_2,\dots, y_{n-1}]^T,$ are called dependent or response variable. Here $n$ is number of samples measured.
\end{defn}
The aim of the regression is to estimate the relationship between $\hat{x}$ and $\hat{y}$ variables in order to make predictions and find functional dependences. \\
Let's assume we measure a set of parameters for each sample. Number of parameters is denoted by $p$. In this case instead for vector we would get a matrix $\textbf{X}$ and $\textbf{Y}$. Matrix $\textbf{X}$ is called \textbf{design matrix} and matrix $\textbf{Y}$ is \textbf{response matrix}. Goal of regression analysis is to determine the functional dependence between $\textbf{X}$ and $\textbf{Y}$ or one may say explain one in terms of the other. Here we have no knowledge on the function is available in advance. However one may assume the function to be linear with respect to some unknown parameters $\beta = (\beta_1, \ldots, \beta_p)^{\top}$.Such assumption leads to so called \textbf{linear regression} and set of $\beta$  are \textbf{regression parameters}. There are many types of linear regressions and in the sections below we are going to describe some of them.
\subsection{Ordinary Least Squared}

\subsection{Ridge}

\subsection{Lasso}

\subsection{Resampling methods}\label{resampling}
As we have already mention the main goal of regression analysis is to find the relations between the dependent and independent variables by estimating set of parameters. Once we have sampled the date and run the regression analysis we have obtained this parameters. However we only can do it once for a single data sample and can't predict how the model we have made will explain new data. The process of collecting data might also be expensive both in computational time and in real life currency, so one might need a recipe to avoid collecting more data and re-use the existing. Another problem here is overfitting. If our model provides us an estimate that corresponds too close to the provided set of data (usually called training data) it might fail to predict the fit for any additional data (usually refereed to as testing data). Generally speaking resampling is a method that aims to improve the prediction accuracy of the model. This is achieved by implementing a very simple idea - splitting the data set into training and testing data. There many such methods for example k-fold cross-validation and bootstrapping. \\
K-fold cross-validation require dividing the data set into a set of k-subsets (folds). After doing so one of the subsets is considered as a test one and the remaining ones are train data ($k-1$). This is done for all subsets, so that each of them has to be used as testing data, which means that the process should be repeated $k$ times. Bootstrapping is a bit different - test data set is selected once and used for every selection of the training data. Training data is selected randomly, by taking some values from data set with replacement. Random selection with replacement means that each value may appear multiple times in the one sample.
 
\subsection{Bias-variance tradeoff}

As it was mentioned in $\ref{resampling}$ overfitting is a problem we want to avoid. This many be done by reducing the complexity of the model. However, it may lead to another problem - underfitting, which is means that we are ignoring some important features of the train data. The problem of balancing these two is called bias-variance tradeoff or bias-variance dilemma. This is a well known problem in data analysis. Here high bias correspond to underfitting and high variance to overfitting. High bias usually means that the model is not complex enough and correspondingly high variance means that model is too complex. \\
The higher complexity of the model means that we have better approximation and that lead to lower bias. However any real model has noise and higher approximation is not always good, as we want to avoid the influence of noise. Reducing complexity we obtain lower approximation and higher variance. The best case scenario here is to balance bias and variance.  

\begin{eqnarray}
	E[(y - \hat f(x))^2] = \text{Bias}(\hat f(x))^2 + \text{Var}(\hat f(x)) + \sigma^2 \\ = \left(E[f(x) - \hat f(x)]^2\right)+ \left(E[\hat f(x)^2] - E[\hat f(x)]^2\right) + \sigma^2
\end{eqnarray}



\section{Results and discussion} \label{Results}

\section{Conclusion}\label{Conclusion}

\end{document}
